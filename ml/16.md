# 第十六章 从零创建 KNN 分类器：第一部分

> 原文：[Creating a K Nearest Neighbors Classifer from scratch](https://pythonprogramming.net/programming-k-nearest-neighbors-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/wizardforcel)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

欢迎阅读第十六个部分，我们现在涉及到 KNN 算法的分类。在上一个教程中，我们涉及到了欧氏距离，现在我们开始使用纯粹的 Python 代码来建立我们自己的简单样例。

最开始，让我们导入下列东西并为 Matplotlib 设置一个样式。

```py
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
import warnings
from math import sqrt
from collections import Counter
style.use('fivethirtyeight')
```

我们打算使用警告来避免使用比分组数量更低的 K 值，至少是最开始（因为我会展示一个更加高效的方法），之后对集合计数来获取出现次数最多的分类。

下面，我们创建一些数据：

```py
dataset = {'k':[[1,2],[2,3],[3,1]], 'r':[[6,5],[7,7],[8,6]]}
new_features = [5,7]
```

这个数据集只是个 Python 字典，键是点的颜色（将这些看做分类），值是属于这个分类的数据点。如果你回忆我们的乳腺肿瘤数据集，分类都是数字，通常 Sklearn 只能处理数字。例如，向量翻译为任意数字`2`，而恶性翻译为任意数字`4`，而不是一个字符串。这是因为，Sklearn 只能使用数字，但是你并不一定要使用数字来代表分类。下面，我们创建简单的数据集`5, 7`，用于测试。我们可以这样来快速绘图：

```py
[[plt.scatter(ii[0],ii[1],s=100,color=i) for ii in dataset[i]] for i in dataset]
plt.scatter(new_features[0], new_features[1], s=100)

plt.show()
```

![](img/16-1.png)

` [[plt.scatter(ii[0],ii[1],s=100,color=i) for ii in dataset[i]] for i in dataset]`这一行和下面这个相同：

```py
for i in dataset:
    for ii in dataset[i]:
        plt.scatter(ii[0],ii[1],s=100,color=i)
```

你可以看到红色和黑色的明显分组，并且我们还有蓝色的点，它是`new_features`，我们打算对其分类。

我们拥有了数据，现在我们打算创建一些函数，来分类数据。

```py
def k_nearest_neighbors(data, predict, k=3):

    return vote_result
```

这就是我们的框架，从这里开始。我们想要一个函数，它接受要训练的数据，预测的数据，和 K 值，它的默认值为 3。

下面，我们会开始填充函数，首先是一个简单的警告：


```py
def k_nearest_neighbors(data, predict, k=3):
    if len(data) >= k:
        warnings.warn('K is set to a value less than total voting groups!')

    return vote_result
```

如果选取的最近邻的数量小于或等于分类数量，那么就给出警告（因为这样会产生偏差）。

现在，如何寻找最近的三个点呢？是否有一些用于搜索的魔法呢？没有，如果有的话，也是很复杂而。为什么呢？KNN 的工作原理是，我们需要将问题中的数据与之前的数据比较，之后才能知道最近的点是什么。因此，如果你的数据越多，KNN 就越慢。我们这里告一段落，但是要考虑是否有方法来加速这个过程。
