# 第十一章 回归 - 编程计算 R 平方

> 原文：[Regression - How to Program R Squared](https://pythonprogramming.net/how-to-program-r-squared-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/wizardforcel)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

欢迎阅读第十一篇教程。既然我们知道了我们寻找的东西，让我们实际在 Python 中计算它吧。第一步就是计算平方误差。函数可能是这样：

```py
def squared_error(ys_orig,ys_line):
    return sum((ys_line - ys_orig) * (ys_line - ys_orig))
```

使用上面的函数，我们可以计算出任何实现到数据点的平方误差。所以我们可以将这个语法用于回归直线和 y 均值直线。也就是说，平方误差只是判定系数的一部分，所以让我们构建那个函数吧。由于平方误差函数只有一行，你可以选择将其嵌入到判定系数函数中，但是平方误差是你在这个函数之外计算的东西，所以我选择将其单独写成一个函数。对于 R 平方：

```py
def coefficient_of_determination(ys_orig,ys_line):
    y_mean_line = [mean(ys_orig) for y in ys_orig]
    squared_error_regr = squared_error(ys_orig, ys_line)
    squared_error_y_mean = squared_error(ys_orig, y_mean_line)
    return 1 - (squared_error_regr/squared_error_y_mean)
```

我们所做的是，计算 y 均值直线，使用单行的`for`循环（其实是不必要的）。之后我们计算了 y 均值的平方误差，以及回归直线的平方误差，使用上面的函数。现在，我们需要做的就是计算出 R 平方之，它仅仅是 1 减去回归直线的平方误差，除以 y 均值直线的平方误差。我们返回该值，然后就完成了。组合起来并跳过绘图部分，代码为：

```py
from statistics import mean
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')

xs = np.array([1,2,3,4,5], dtype=np.float64)
ys = np.array([5,4,6,5,6], dtype=np.float64)

def best_fit_slope_and_intercept(xs,ys):
    m = (((mean(xs)*mean(ys)) - mean(xs*ys)) /
         ((mean(xs)*mean(xs)) - mean(xs*xs)))
    b = mean(ys) - m*mean(xs)
    return m, b

def squared_error(ys_orig,ys_line):
    return sum((ys_line - ys_orig) * (ys_line - ys_orig))

def coefficient_of_determination(ys_orig,ys_line):
    y_mean_line = [mean(ys_orig) for y in ys_orig]
    squared_error_regr = squared_error(ys_orig, ys_line)
    squared_error_y_mean = squared_error(ys_orig, y_mean_line)
    return 1 - (squared_error_regr/squared_error_y_mean)
    
m, b = best_fit_slope_and_intercept(xs,ys)
regression_line = [(m*x)+b for x in xs]

r_squared = coefficient_of_determination(ys,regression_line)
print(r_squared)
# 0.321428571429

##plt.scatter(xs,ys,color='#003F72',label='data')
##plt.plot(xs, regression_line, label='regression line')
##plt.legend(loc=4)
##plt.show()
```

这是个很低的值，所以根据这个度量，我们的最佳拟合直线并不是很好。这里的 R 平方是个很好的度量手段吗？可能取决于我们的目标。多数情况下，如果我们关心准确预测未来的值，R 平方的确很有用。如果你对预测动机或者趋势感兴趣，我们的最佳拟合直线实际上已经很好了。R 平方不应该如此重要。看一看我们实际的数据集，我们被一个较低的数值卡住了。值与值之间的变化在某些点上是 20% ~ 50%，这已经非常高了。我们完全不应该感到意外，使用这个简单的数据集，我们的最佳拟合直线并不能描述真实数据。

但是，我们刚才说的是一个假设。虽然我们逻辑上统一这个假设，我们需要提出一个新的方法，来验证假设。到目前为止的算法非常基础，我们现在只能做很少的事情，所以没有什么空间来改进误差了，但是之后，你会在空间之上发现空间。不仅仅要考虑算法本身的层次空间，还有由很多算法层次组合而成的算法。其中，我们需要测试它们来确保我们的假设，关于算法是干什么用的，是正确的。考虑把操作组成成函数由多么简单，之后，从这里开始，将整个验证分解成数千行代码。

我们在下一篇教程所做的是，构建一个相对简单的数据集生成器，根据我们的参数来生成数据。我们可以使用它来按照意愿操作数据，之后对这些数据集测试我们的算法，根据我们的假设修改参数，应该会产生一些影响。我们之后可以将我们的假设和真实情况比较，并希望他们匹配。这里的例子中，假设是我们正确编写这些算法，并且判定系数低的原因是，y 值的方差太大了。我们会在下一个教程中验证这个假设。
