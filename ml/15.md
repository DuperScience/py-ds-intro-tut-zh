# 第十五章 对数据使用 KNN

> 原文：[Euclidean Distance theory](https://pythonprogramming.net/euclidean-distance-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/wizardforcel)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

欢迎阅读第十五篇教程，其中我们当前涉及到使用 KNN 算法来分类。上一篇教程中，我们涉及到如何使用 Sklearn 的 KNN 算法来预测良性或者恶性肿瘤，基于肿瘤的属性，准确率有 95%。现在，我们打算深入 KNN 的工作原理，以便完全理解算法本身，来使其更好为我们工作。

我们会回到我们的乳腺肿瘤数据集，对其使用我们自定义 KNN 算法，并将其与 Sklearn 的比较，但是我们打算首先以一个非常简单的理论开始。KNN 基于近似性，不是分组，而是单独的点。所以，所有这种算法所做的，实际上是计算点之间的距离，并且之后选取距离最近的前 K 个点的最常出现的分类。有几种方式来计算平面上的距离，他们中许多都可以在这里使用，但是最常使用的版本是欧氏距离，以欧几里得命名。他是一个著名的数学家，被称为几何之父，他编写了《几何原本》，被称为数学家的圣经。欧氏距离为：

![](img/15-1.png)

所以这是什么意思？基本上，它是每个点之间距离的平方和的平方根。在 Python 的术语中，是这样：

```py
plot1 = [1,3]
plot2 = [2,5]
euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 )
```

这里距离是 2.236。

这就是 KNN 背后的基本数学原理了，现在我们仅仅需要构建一个系统来处理算法的剩余部分，例如寻找最近距离，它们的分组，然后是计数。
