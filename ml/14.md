# 第十四章 对数据使用 KNN

> 原文：[Applying K Nearest Neighbors to Data](https://pythonprogramming.net/k-nearest-neighbors-application-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/wizardforcel)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

欢迎阅读第十四个部分。上一个部分我们介绍了分类，它是一种机器学习的监督算法，并且解释了 KNN 算法的直觉。这个教程中，我们打算使用 Sklearn，讲解一个简单的算法示例，之后在后面的教程中，我们会构建我们自己的算法来更多了解背后的工作原理。

为了使用例子说明分类，我们打算使用[乳腺肿瘤数据集](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29)，它是 UCI 所贡献的数据集，从威斯康星大学收集。UCI 拥有庞大的[机器学习仓库](https://archive.ics.uci.edu/ml/datasets.html)。这里的数据集组织为经常使用的机器学习算法类型、数据类型、属性类型、主题范围以及其它。它们对教学和机器学习算法开发都很实用。我自己经常浏览那里，非常值得收藏。在乳腺肿瘤数据集的页面，选择[`Data Folder`链接](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)。之后，下载`breast-cancer-wisconsin.data`和`breast-cancer-wisconsin.names`。这些可能不能下载，而是会在浏览器中展示。如果是这样右键点击“另存为”。

下载之后，打开`breast-cancer-wisconsin.names`文件。查看文件，向下滚动 100 行，我们就能获取属性（列）的名称、使用这些信息，我们打算手动将这些标签添加到` breast-cancer-wisconsin.data`文件中。打开它，并输入新的第一行：

```
id,clump_thickness,uniform_cell_size,
uniform_cell_shape,marginal_adhesion,
single_epi_cell_size,bare_nuclei,bland_chromation,
normal_nucleoli,mitoses,class
```

之后，你应该会思考，我们的特征和标签应该是什么。我们尝试对数据进行分类，所以很显然分类就是这些属性会导致良性还是恶性。同样，大多数这些属性看起来都是可用的，但是是否有任何属性与其它属性类似，或者是无用的？ID 属性并不是我们打算扔给分类器的东西。

缺失或者损坏的数据：这个数据集拥有一些缺失数据，我们需要清理。让我们以导入来开始，拉取数据，之后做一些清理：

```py
import numpy as np
from sklearn import preprocessing, cross_validation, neighbors
import pandas as pd

df = pd.read_csv('breast-cancer-wisconsin.data.txt')
df.replace('?',-99999, inplace=True)
df.drop(['id'], 1, inplace=True)
```

在读取数据之后，我们注意到，有一些列存在缺失数据。这些缺失数据以`?`填充。`.names`文件告诉了我们，但是我们最终可以通过错误来发现，如果我们尝试将这些信息扔给分类为。这个时候，我们选择将缺失数据填充为 -99999 值。你可以选择你自己的方法来处理缺失数据，但是在真实世界中，你可能发现 50% 或者更多记录，一个或多个列都含有缺失数据。尤其是，如果你使用可扩展的属性来收集数据。-99999 并不完美，但是它足够有效了。下面，我们丢弃了 ID 列。完成之后，我们会注释掉 ID 列的丢弃，只是看看包含他可能有哪些影响。

下面，我们定义我们的特征和标签。

特征`X`是除了分类的任何东西。调用`df.drop`会返回一个新的 DataFrame，不带丢弃的列。标签`y`仅仅是分类列。

现在我们创建训练和测试样本，使用 Sklearn 的`cross_validation.train_test_split`。

```py
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)
```

定义分类器：


```py
clf = neighbors.KNeighborsClassifier()
```

这里，我们使用 KNN 分类器。

训练分类器：

```py
clf.fit(X_train, y_train)
```

测试：

```py
accuracy = clf.score(X_test, y_test)
print(accuracy)
```

结果应该是 95%，并且开箱即用，无需任何调整。非常棒。让我们展示一下，当我们注释掉 ID 列，包含一些无意义和误导性的数据之后，会发生什么。

```py
import numpy as np
from sklearn import preprocessing, cross_validation, neighbors
import pandas as pd

df = pd.read_csv('breast-cancer-wisconsin.data.txt')
df.replace('?',-99999, inplace=True)
#df.drop(['id'], 1, inplace=True)

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)

clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
accuracy = clf.score(X_test, y_test)
print(accuracy)
```

影响很令人吃惊，准确率从 95% 降到了 60%。在未来，如果 AI 通知了这个星球，要注意你只需要给它一些无意义的属性来智取它。非常有意思，添加噪声是一种损害你的算法的方式。当你和你的机器人霸主较量时，分辨有意义和恶意的噪声会节省你的时间。


下面你可以大致猜测，我们如何做预测，如果你遵循了 Sklearn 的教程。首先，我们需要一些沿革本数据。我们可以自己编。例如，我们会查看样本文件的某一行。你可以添加噪声来执行进一步的分析，假设标准差不是那么离谱。这么做也比较安全，由于你并不在篡改的数据上训练，你仅仅做了测试。我会通过编造一行来手动实现它。

```py
example_measures = np.array([4,2,1,1,1,2,3,2,1])
```

你可以尽管在文档中搜索特征列表。它不存在。现在你可以：

```py
prediction = clf.predict(example_measures)
print(prediction)
```

或者取决于你的阅读时间，你可能不能这么做。在这么做的时候，我得到了一个警告：

```py
DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
```

好的，没有问题。我们只拥有一个特征吗？不是。我们只拥有一个记录吗？是的。所以我们使用`X.reshape(1, -1)`。

```py
example_measures = np.array([4,2,1,1,1,2,3,2,1])
example_measures = example_measures.reshape(1, -1)
prediction = clf.predict(example_measures)
print(prediction)
# 0.95
# [2]
```

这里的第一个输出是准确率（95%）和预测（2）。这就是我们的伪造数据的建模。

如果我们有两条呢？

```py
example_measures = np.array([[4,2,1,1,1,2,3,2,1],[4,2,1,1,1,2,3,2,1]])
example_measures = example_measures.reshape(2, -1)
prediction = clf.predict(example_measures)
print(prediction)
```

忽略这个硬编码。如果我们不知道有几何样例会怎么样？

```py
example_measures = np.array([[4,2,1,1,1,2,3,2,1],[4,2,1,1,1,2,3,2,1]])
example_measures = example_measures.reshape(len(example_measures), -1)
prediction = clf.predict(example_measures)
print(prediction)
```

你可以看到，KNN 算法的实现不仅仅很简单，而且这里也很准确。下一个教程中，我们打算从零构建我们自己的 KNN 算法，而不是使用 Sklearn，来尝试了解更多算法的东西，理解它的工作原理，最重要的是，了解它的陷阱。
