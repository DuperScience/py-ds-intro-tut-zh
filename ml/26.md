# 第二十六章 支持向量机优化

> 原文：[Support Vector Machine Optimization in Python](https://pythonprogramming.net/svm-optimization-python-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/wizardforcel)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

欢迎阅读第二十六篇教程，下面就是我们的支持向量机章节。这篇教程中，我们打算处理 SVM 的优化方法`fit`。

目前为止的代码为：

```py
import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np
style.use('ggplot')

class Support_Vector_Machine:
    def __init__(self, visualization=True):
        self.visualization = visualization
        self.colors = {1:'r',-1:'b'}
        if self.visualization:
            self.fig = plt.figure()
            self.ax = self.fig.add_subplot(1,1,1)
    # train
    def fit(self, data):
        pass

    def predict(self,features):
        # sign( x.w+b )
        classification = np.sign(np.dot(np.array(features),self.w)+self.b)

        return classification
        
data_dict = {-1:np.array([[1,7],
                          [2,8],
                          [3,8],]),
             
             1:np.array([[5,1],
                         [6,-1],
                         [7,3],])}
```

我们开始填充`fit`方法：

```py
    def fit(self, data):
        self.data = data
        # { ||w||: [w,b] }
        opt_dict = {}

        transforms = [[1,1],
                      [-1,1],
                      [-1,-1],
                      [1,-1]]
```

要注意这个方法首先传递`self`（记住这是方法的约定），之后传递`data`。`data`就是我们我们打算训练或者优化的数据。我们这里，它是`data_dict`，我们已经创建好了。

我们将`self.data`设为该数据。现在，我们可以在类中的任何地方引用这个训练数据了（但是，我们需要首先使用数据来调用这个训练方法，来避免错误）。

下面，我们开始构建最优化字典`opt_dict`，它包含任何最优化的值。随着我们减小我们的`w`向量，我们会使用约束函数来测试向量，如果存在的话，寻找最大的满足方程的`b`，之后将所有数据储存在我们的最华友字典中。字典是`{ ||w|| : [w,b] }`。当我们完成所有优化时，我们会选择字典中键最小的`w`和`b`值。

最后，我们会设置我们的转换。我们已经解释了我们的意图，来确保我们检查了每个可能的向量版本。

下面，我们需要一些匹配数据的起始点。为此，我们打算首先引用我们的训练数据，来选取一些合适的起始值。


```py
        # finding values to work with for our ranges.
        all_data = []
        for yi in self.data:
            for featureset in self.data[yi]:
                for feature in featureset:
                    all_data.append(feature)

        self.max_feature_value = max(all_data)
        self.min_feature_value = min(all_data)
        # no need to keep this memory.
        all_data=None
```

我们所做的就是遍历所有数据，寻找最大值和最小值。现在我们打算定义我们的步长。

```py
        step_sizes = [self.max_feature_value * 0.1,
                      self.max_feature_value * 0.01,
                      # starts getting very high cost after this.
                      self.max_feature_value * 0.001]
```

这里我们设置了一些大小的步长，我们打算这样执行。对于我们的第一遍，我们会采取大跨步（10%）。一旦我们使用这些步长找到了最小值，我们就将步长降至 1% 来调优。我们会继续下降，取决于你想要多么精确。我会在这个项目的末尾讨论，如何在程序中判断是否应该继续优化。

下面，我们打算设置一些变量，来帮助我们给`b`生成步长（用于生成比`w`更大的步长，因为我们更在意`w`的精确度），并跟踪最后一个最优值。

```py
        # extremely expensive
        b_range_multiple = 5
        b_multiple = 5
        latest_optimum = self.max_feature_value*10
```

现在我们开始了：


```py
        for step in step_sizes:
            w = np.array([latest_optimum,latest_optimum])
            # we can do this because convex
            optimized = False
            while not optimized:
                pass
```

这里的思想就是沿着向量下降。开始，我们将`optimized`设为`False`，并为我们会在每个主要步骤重置它。`optimized`变量再我们检查所有步骤和凸形状（我们的碗）的底部之后，会设为`True`。

我们下个教程中会继续实现这个逻辑，那里我们会实际使用约束问题来检查值，检查我们是否找到了可以保存的值。

目前为止的代码：

```py
import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np
style.use('ggplot')

class Support_Vector_Machine:
    def __init__(self, visualization=True):
        self.visualization = visualization
        self.colors = {1:'r',-1:'b'}
        if self.visualization:
            self.fig = plt.figure()
            self.ax = self.fig.add_subplot(1,1,1)
    # train
    def fit(self, data):
        self.data = data
        # { ||w||: [w,b] }
        opt_dict = {}

        transforms = [[1,1],
                      [-1,1],
                      [-1,-1],
                      [1,-1]]

        all_data = []
        for yi in self.data:
            for featureset in self.data[yi]:
                for feature in featureset:
                    all_data.append(feature)

        self.max_feature_value = max(all_data)
        self.min_feature_value = min(all_data)
        all_data = None

        step_sizes = [self.max_feature_value * 0.1,
                      self.max_feature_value * 0.01,
                      # point of expense:
                      self.max_feature_value * 0.001,]
        
        # extremely expensive
        b_range_multiple = 5
        # 
        b_multiple = 5
        latest_optimum = self.max_feature_value*10

        for step in step_sizes:
            w = np.array([latest_optimum,latest_optimum])
            # we can do this because convex
            optimized = False
            while not optimized:
                pass
            
    def predict(self,features):
        # sign( x.w+b )
        classification = np.sign(np.dot(np.array(features),self.w)+self.b)

        return classification
        


data_dict = {-1:np.array([[1,7],
                          [2,8],
                          [3,8],]),
             
             1:np.array([[5,1],
                         [6,-1],
                         [7,3],])}
```
