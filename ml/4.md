# 第四章 回归 - 训练和测试

> 原文：[Regression - Training and Testing](https://pythonprogramming.net/training-testing-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

欢迎阅读 Python 机器学习系列教程的第四部分。在上一个教程中，我们获取了初始数据，按照我们的喜好操作和转换数据，之后我们定义了我们的特征。Scikit 不需要处理 Pandas 和 DataFrame，我出于自己的喜好而处理它，因为它快并且高效。反之，Sklearn 实际上需要 NumPy 数组。Pandas 的 DataFrame 可以轻易转换为 NumPy 数组，所以事情就是这样的。

目前为止我们的代码：

```py
import quandl, math
import numpy as np
import pandas as pd
from sklearn import preprocessing, cross_validation, svm
from sklearn.linear_model import LinearRegression

df = quandl.get("WIKI/GOOGL")

print(df.head())
#print(df.tail())

df = df[['Adj. Open',  'Adj. High',  'Adj. Low',  'Adj. Close', 'Adj. Volume']]

df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Close'] * 100.0
df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0

df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']]
print(df.head())

forecast_col = 'Adj. Close'
df.fillna(value=-99999, inplace=True)
forecast_out = int(math.ceil(0.01 * len(df)))

df['label'] = df[forecast_col].shift(-forecast_out)
```

我们之后要丢弃所有仍旧是 NaN 的信息。

```py
df.dropna(inplace=True)
```

对于机器学习来说，通常要定义`X`（大写）作为特征，和`y`（小写）作为对于特征的标签。这样，我们可以定义我们的特征和标签，像这样：

```py
X = np.array(df.drop(['label'], 1))
y = np.array(df['label'])
```

上面，我们所做的就是定义`X`（特征），是我们整个的 DataFrame，除了`label`列，并转换为 NumPy 数组。我们使用`drop`方法，可以用于 DataFrame，它返回一个新的 DataFrame。下面，我们定义我们的`y`变量，它是我们的标签，仅仅是 DataFrame 的标签列，并转换为 NumPy 数组。

现在我们就能告一段落，转向训练和测试了，但是我们打算做一些预处理。通常，你希望你的特征在 -1 到 1 的范围内。这可能不起作用，但是通常会加速处理过程，并有助于准确性。因为大家都使用这个范围，它包含在了 Sklearn 的`preprocessing`模块中。为了使用它，你需要对你的`X`变量调用` preprocessing.scale`。

```py
X = preprocessing.scale(X)
```

下面，创建标签`y`：

```py
y = np.array(df['label'])
```

现在就是训练和测试的时候了。方式就是选取 75% 的数据用于训练机器学习分类器。之后选取剩下的 25% 的数据用于测试分类器。由于这是你的样例数据，你应该拥有特征和一直标签。因此，如果你测试后 25% 的数据，你就会得到一种准确度和可靠性，叫做置信度。有许多方式可以实现它，但是，最好的方式可能就是使用内建的`cross_validation`，因为它也会为你打乱数据。代码是这样：

```py
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)
```

这里的返回值是特征的训练集、测试集、标签的训练集和测试集。现在，我们已经定义好了分类器。Sklearn 提供了许多通用的分类器，有一些可以用于回归。我们会在这个例子中展示一些，但是现在，让我们使用`svm`包中的支持向量回归。

```py
clf = svm.SVR()
```

我们这里仅仅使用默认选项来使事情简单，但是你可以在[`sklearn.svm.SVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)的文档中了解更多。

一旦你定义了分类器，你就可以训练它了。在 Sklearn 中，使用`fit`来训练。

```py
clf.fit(X_train, y_train)
```

这里，我们拟合了我们的训练特征和训练标签。

我们的分类器现在训练完毕。这非常简单，现在我们可以测试了。

```py
confidence = clf.score(X_test, y_test)
```

加载测试，之后：

```py
print(confidence)
# 0.960075071072
```

所以这里，我们可以看到准确率几乎是 96%。没有什么可说的，让我们尝试另一个分类器，这一次使用`LinearRegression`：

```py
clf = LinearRegression()
# 0.963311624499
```

更好一点，但是基本一样。所以作为科学家，我们如何知道，选择哪个算法呢？不久，你会熟悉什么在多数情况下都工作，什么不工作。你可以从 Scikit 的站点上查看[选择正确的评估工具](https://scikit-learn.org/stable/tutorial/machine_learning_map/)。这有助于你浏览一些基本的选项。如果你询问搞机器学习的人，它完全是试验和出错。你会尝试大量的算法并且仅仅选取最好的那个。要注意的另一件事情就是，一些算法必须线性运行，其它的不是。不要把线性回归和线性运行搞混了。所以这些意味着什么呢？一些机器学习算法会一次处理一步，没有多线程，其它的使用多线程，并且可以利用你机器上的多核。你可以深入了解每个算法，来弄清楚哪个可以多线程，或者你可以阅读文档，并查看`n_jobs`参数。如果拥有`n_jobs`，你就可以让算法通过多线程来获取更高的性能。如果没有，就很不走运了。所以，如果你处理大量的数据，或者需要处理中等规模的数据，但是需要很高的速度，你就可能想要线程加速。让我们看看这两个算法。

访问[`sklearn.svm.SVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)的文档，并查看参数，看到`n_jobs`了嘛？反正我没看到，所以它就不能使用线程。你可能会看到，在我们的小型数据集上，差异不大。但是，假设数据集由 20MB，差异就很明显。然后，我们查看`LinearRegression`算法，看到`n_jobs`了嘛？当然，所以这里，你可以指定你希望多少线程。如果你传入`-1`，算法会使用所有可用的线程。

这样：

```py
clf = LinearRegression(n_jobs=-1)
```

就够了。虽然我让你做了很少的事情（查看文档），让我给你说个事实吧，仅仅由于机器学习算法使用默认参数工作，不代表你可以忽略它们。例如，让我们回顾`svm.SVR`。SVR 是支持向量回归，在执行机器学习时，它是一种架构。我非常鼓励那些有兴趣学习更多的人，去研究这个主题，以及向比我学历更高的人学习基础。我会尽力把东西解释得更简单，但是我并不是专家。回到刚才的话题，`svm.SVR`有一个参数叫做`kernel`。这个是什么呢？核就相当于你的数据的转换。这使得处理过程更加迅速。在`svm.SVR`的例子中，默认值是`rbf`，这是核的一个类型，你有一些选择。查看文档，你可以选择`'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'`或者一个可调用对象。同样，就像尝试不同的 ML 算法一样，你可以做你想做的任何事情，尝试一下不同的核吧。

```py
for k in ['linear','poly','rbf','sigmoid']:
    clf = svm.SVR(kernel=k)
    clf.fit(X_train, y_train)
    confidence = clf.score(X_test, y_test)
    print(k,confidence)
```

```py
linear 0.960075071072
poly 0.63712232551
rbf 0.802831714511
sigmoid -0.125347960903
```

我们可以看到，线性的核表现最好，之后是`rbf`，之后是`poly`，`sigmoid`很显然是个摆设，并且应该移除。

所以我们训练并测试了数据集。我们已经有 71% 的满意度了。下面我们做什么呢？现在我们需要再进一步，做一些预测，下一章会涉及它。
