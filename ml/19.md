# 第十九章 KNN 的最终见解

> 原文：[Final thoughts on K Nearest Neighbors](https://pythonprogramming.net/final-thoughts-knn-machine-learning-tutorial/)

> 译者：[飞龙](https://github.com/wizardforcel)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

既然我们了解了它的工作原理，这里我们打算涉及一些 KNN 算法的最终见解，包含 K 值，置信度，速度，以及算法的优点和缺点。

在执行 100 个样例的测试之后，Sklearn 的`neighbors.KNeighborsClassifier`分类器的准确率是 0.97，我们自己编写的分类器也一样。不要故步自封，因为这个算法非常简单和基础。KNN 分类器的真正价值并不在准确率上，而是它的速度。KNN 分类器的主要缺陷就是就是速度，你可以用它来执行操作。

对于速度，Sklearn 的 KNN 版本的每个周期是 0.044 秒，我们的是 0.55 秒。因此，虽然我们实现了相同的结果，我们比 Sklearn 慢很多。好的消息是，如果你对它们如何实现的感兴趣，你可以查看源代码、我们也提到了，我们也可以使用一个主流方式来提升速度。KNN 并不需要过多的训练。训练仅仅是将数据集加载到内存。你可以将数据集保留在内存中，但是 KNN 分类器的真正痛点就是对比每个数据集来寻找最近的那个。之后，如果你打算对 1000 个数据集分类，会发生什么呢？是的，一个选项是可以并发。串行执行它们没有任何好处。我们的方式是这样，仅仅使用一点点的处理器的能力。但是，我们可以一次性至少计算 100~200 个数据，即使是在便宜的处理器上。如果你打算了解如何并发，看一看这个[并发教程](https://pythonprogramming.net/threading-tutorial-python/)。使用 Sklearn，KNN 分类器自带一个并行处理参数`n_jobs`。你可以将其设置为任何数值，你可以以这个线程数来并发。如果你打算一次运行 100 个操作，`n_jobs=100`。如果你仅仅打算运行尽可能做的操作，设置`n_jobs=-1`。阅读[最近邻文档](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)，你可以了解更多选项。有几种方式将你的数据与指定半径之内的数据对比，如果你对加速 KNN，以及 Sklearn 的 KNN 版本感兴趣，你可能想要看一看。

最后，我要讲的最后一点就是预测的置信度。有两种方式来度量置信度。一种是比较你预测对了多少个点，另一个是，检查计数的百分比。例如，你的算法准确率可能是 97%，但是对于一些分类，计数可能是 3 比 2。其中 3 是主流，它的占有率是 60%，而不是理想情况下的 100%。但是告诉别人它是否有癌症的话，就像自动驾驶汽车分辨一团土和毛毯上的孩子，你可能更希望是 100%。可能 60% 的计数就是 3% 的不准确度量的一部分。

好的，所以我们刚刚编写了准确率为 97% 的分类器，但是没有把所有事情都做好。KNN 非常拥有，因为它对线性和非线性数据都表现出色。主要的缺陷是规模、离群点和不良数据（要记得 ID 列的无效引入）。

我们仍然专注于监督式机器学习，特别是分类，我们下面打算设计支持向量机。

最后的代码：

```py
import numpy as np
from math import sqrt
import warnings
from collections import Counter
import pandas as pd
import random

def k_nearest_neighbors(data, predict, k=3):
    if len(data) >= k:
        warnings.warn('K is set to a value less than total voting groups!')
    distances = []
    for group in data:
        for features in data[group]:
            euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))
            distances.append([euclidean_distance, group])

    votes = [i[1] for i in sorted(distances)[:k]]
    vote_result = Counter(votes).most_common(1)[0][0]
    confidence = Counter(votes).most_common(1)[0][1] / k
  
    return vote_result, confidence


df = pd.read_csv("breast-cancer-wisconsin.data.txt")
df.replace('?',-99999, inplace=True)
df.drop(['id'], 1, inplace=True)
full_data = df.astype(float).values.tolist()
random.shuffle(full_data)

test_size = 0.4
train_set = {2:[], 4:[]}
test_set = {2:[], 4:[]}
train_data = full_data[:-int(test_size*len(full_data))]
test_data = full_data[-int(test_size*len(full_data)):]

for i in train_data:
    train_set[i[-1]].append(i[:-1])
    
for i in test_data:
    test_set[i[-1]].append(i[:-1])

correct = 0
total = 0

for group in test_set:
    for data in test_set[group]:
        vote,confidence = k_nearest_neighbors(train_set, data, k=5)
        if group == vote:
            correct += 1
        total += 1
print('Accuracy:', correct/total)
```
